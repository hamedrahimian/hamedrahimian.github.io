{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Probability\n",
    "\n",
    "* An **experiment** is any activity or process whose outcome is subject to uncertainty\n",
    "\n",
    "* The **sample space** of an experiment, denoted by $\\mathcal{S}$, is the set of all possible\n",
    "outcomes of that experiment.\n",
    "\n",
    "* An event is any collection (subset) of outcomes contained in the sample space $\\mathcal{S}$. \n",
    " \n",
    "## Example:  \n",
    "Consider an experiment in which each of **three** vehicles taking a particular freeway\n",
    "exit turns left (L) or right (R) at the end of the exit ramp. The **eight** possible outcomes\n",
    "that comprise the sample space are \n",
    "$$\\mathcal{S}=\\{LLL, RLL, LRL, LLR, LRR, RLR, RRL, and RRR\\}.$$\n",
    "Thus, there are eight simple events, among which are $E1=\\{LLL\\}$ and $E5=\\{LRR\\}$.\n",
    "Some compound events include \n",
    "\n",
    " + $A=\\{RLL, LRL, LLR\\}$: the event that exactly one of the three\n",
    "vehicles turns right; \n",
    "\n",
    " + $B=\\{LLL, RLL, LRL, LLR\\}$:  the event that at most one of the\n",
    "vehicles turns right; \n",
    "\n",
    " + $C=\\{LLL, RRR\\}$: the event that all three vehicles turn in the\n",
    "same direction. \n",
    "\n",
    "\n",
    "# Axioms of Probability\n",
    "\n",
    "* For any event $A$, $P(A)\\ge 0$.\n",
    "* $P(\\mathcal{S})=1$.\n",
    "* If $A_1$, $A_2$, $A_3$, $\\ldots$ is an infinite collection of disjoint events, then $P(A_1 \\cup  A_2 \\cup A_3 \\cup \\ldots)=\\sum_{i=1}^{\\infty} P(A_i)$. \n",
    "\n",
    "*Note*: Events $A_1$, $A_2$, $A_3$, $\\ldots$  are called *disjoint* or *mutually exclusive* if for any pair of events $A_{i}$ and $A_{j}$, we have $A_i \\cap A_j=\\emptyset$.  \n",
    "\n",
    "\n",
    "\n",
    "# Conditional Probability\n",
    "\n",
    "For any two events $A$ and $B$ with $P(B)> 0$, the conditional probability of $A$\n",
    "given that $B$ has occurred is defined by\n",
    "\\begin{equation*}\n",
    "    P(A|B)=\\frac{P(A\\cap B)}{P(B)}.\n",
    "\\end{equation*}\n",
    "\n",
    "# The Law of Total Probability\n",
    "\n",
    "Let $A_1, \\ldots, A_k$ be mutually exclusive and exhaustive events. Then for any other event $B$,\n",
    "\\begin{equation*}\n",
    "    P(B)=\\sum_{i=1}^{k} P(B|A_i)P(A_i).\n",
    "\\end{equation*}\n",
    "\n",
    "*Note*: Events $A_1$, $A_2$, $A_3$, $\\ldots$, $A_k$  are called *exhaustive* if $A_1 \\cup  A_2 \\cup A_3 \\cup \\ldots \\cup A_k =\\mathcal{S}$. \n",
    "\n",
    "\n",
    "# Bayes' Theorem\n",
    "\n",
    "Let $A_1, \\ldots, A_k$ be a collection of $k$ mutually exclusive and exhaustive events\n",
    "with prior probabilities $P(A_i)$, $i=1, \\ldots, k$. Then for any other event $B$ for\n",
    "which $P(B) \\ge 0$, the posterior probability of $A_j$, $j=1, \\ldots, k$, given that $B$ has occurred is\n",
    "\\begin{align*}\n",
    "    P(A_j|B) & =\\frac{P(A_j \\cap B)}{P(B)} \\\\\n",
    "            & = \\frac{P(B|A_j)P(A_j)}{\\sum_{i=1}^{k} P(B|A_i)P(A_i)}.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "# Independence\n",
    "\n",
    "* Two events $A$ and $B$ are independent if $P(A | B)= P(A)$ and are dependent\n",
    "otherwise. \n",
    "\n",
    "* The above definition is symmetric. That is $P(B | A)= P(B)$ if and only if (iff) $P(A | B)= P(A)$. \n",
    "\n",
    "* $A$ and $B$ are independent iff \n",
    "$P(A \\cap B)= P(A) \\times  P(B)$. \n",
    "\n",
    "* Events $A_1, \\ldots,  A_n$ are mutually independent if for every $k$, $k=2, \\ldots, n$ and\n",
    "every subset of indices $i_1, i_2, \\ldots, i_k$, we have \n",
    "$$P(A_{i_1} \\cap A_{i_2} \\cap \\ldots A_{i_k}=P(A_{i_1}) P(A_{i_2}) \\ldots P(A_{i_k}).$$\n",
    "\n",
    "# Random Variables\n",
    "\n",
    "* For a given sample space $\\mathcal{S}$ of some experiment, a random variable (rv) is\n",
    "any rule that associates a number with each outcome in $\\mathcal{S}$. In mathematical\n",
    "language, a random variable is a function whose domain is the sample space\n",
    "and whose range is the set of real numbers, i.e., $X : \\mathcal{S} \\mapsto \\mathbb{R}$.\n",
    "\n",
    "## Example\n",
    "\n",
    "When a student calls a university help desk for technical support, he/she will either\n",
    "immediately be able to speak to someone ($S$, for success) or will be placed on hold\n",
    "($F$, for failure). With $\\mathcal{S}=\\{S, F\\}$, define an rv $X$ by\n",
    "$$X(S)=1, \\;  X(F)=0 .$$\n",
    "The rv $X$ indicates whether (1) or not (0) the student can immediately speak to\n",
    "someone.\n",
    "\n",
    "# Types of Random Variables\n",
    "\n",
    "\n",
    "A *discrete* random variable is an rv whose possible values either constitute a finite set or else can be listed in an infinite sequence in which there is a first element, a second element, and so on (\"countably\" infinite).\n",
    "\n",
    "\n",
    "\n",
    "A random variable is *continuous* if both of the following apply:\n",
    "1. Its set of possible values consists either of all numbers in a single interval on the number line (possibly infinite in extent, e.g., from $-\\infty$ to $\\infty$) or all numbers in a disjoint union of such intervals (e.g., $[0, 10] \\cup [20, 30]$).\n",
    "2. No possible value of the variable has positive probability, that is, $P(X = c) =0$ for any possible value $c$.\n",
    "\n",
    "# Probability Distributions\n",
    "\n",
    "\n",
    "* The probability distribution or probability mass function (pmf) of a discrete rv $X$\n",
    "is defined for every number $x$ by $$p(x)= P(X = x)=  P(all \\; \\omega \\in \\mathcal{S} : X(\\omega)= x).$$\n",
    "\n",
    "\n",
    "*Note*: The conditions $p(x) \\ge 0$ and $\\sum_{all \\ possible \\ x} p(x)=1$ are required of any pmf.\n",
    "\n",
    "* Let $X$ be a continuous rv. Then a probability distribution or probability density\n",
    "function (pdf) of $X$ is a function $f(x)$ such that for any two numbers $a$ and\n",
    "$b$ with $a \\le b$,\n",
    "$$P(a \\le X \\le  b) = \\int_{a}^{b} f(x) d x .$$\n",
    "\n",
    "*Note*: For $f(x)$ to be a legitimate pdf, it must satisfy the following two conditions: \n",
    "1. $f (x) \\ge 0$ for all $x$; \n",
    "2. $\\int_{-\\infty}^{\\infty} f(x)dx=1$.\n",
    "\n",
    "\n",
    "# Cumulative Distribution Function \n",
    "\n",
    "The cumulative distribution function  (cdf) of a random variable $X$ is the probability that the observed value pf $X$ is at most $x$: \n",
    "\n",
    "\\begin{equation*}\n",
    "    F(x)=P(X \\le x)= \\begin{cases}\n",
    "        \\sum_{y: y \\le x} p(y)\\\\\n",
    "        \\int_{-\\infty}^{x} f(x)dx.\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "# Expected Value \n",
    "\n",
    "The expected value of a rv $X$ is:\n",
    "\n",
    "\\begin{equation*}\n",
    "    E(X)=\\mu_{X}=\\begin{cases}\n",
    "        \\sum_{x} x p(x)\\\\\n",
    "        \\int_{-\\infty}^{\\infty} x f(x)dx.\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "The expected value of any function $h(X)$ of a rv $X$ is: \n",
    "\n",
    "\\begin{equation*}\n",
    "    E(h(X))=\\begin{cases}\n",
    "        \\sum_{x} h(x) p(x)\\\\\n",
    "        \\int_{-\\infty}^{\\infty} h(x) f(x)dx.\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "# Variance\n",
    "\n",
    "The variance  of a rv $X$ is:\n",
    "\n",
    "\\begin{equation*}\n",
    "    Var(X)=\\sigma^{2}_{X}=E[(X-\\mu)^{2}]= \\begin{cases}\n",
    "        \\sum_{x} (x-\\mu)^{2} p(x)\\\\\n",
    "        \\int_{-\\infty}^{\\infty}  (x-\\mu)^{2} f(x)dx.\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "*Note 1*: $Var(X)=E(X^2)-[E(X)]^2$.\n",
    "\n",
    "*Note 2*: Standard deviation (SD) of $X$ is $\\sigma_{X}=\\sqrt{\\sigma_{X}^{2}}$. \n",
    "\n",
    "# Expected Value and Variance of a Linear Function of $X$\n",
    "\n",
    "* $E(aX+b)=aE(X)+ b$.\n",
    "\n",
    "* $Var(aX+b)=a^2 Var(X)$.\n",
    "\n",
    "# Higher Order Moments\n",
    "\n",
    "* The $k$th moment of the distribution of a rv $X$ is defined as \n",
    "\\begin{equation*}\n",
    "    E(X^k)=\\begin{cases}\n",
    "        \\sum_{x} x^k p(x)\\\\\n",
    "        \\int_{-\\infty}^{\\infty} x^k f(x)dx.\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "* The $k$th centered-moment of the distribution of a rv $X$ is defined as \n",
    "\\begin{equation*}\n",
    "    E[(X-\\mu)^k]=\\begin{cases}\n",
    "        \\sum_{x} (x-\\mu)^k p(x)\\\\\n",
    "        \\int_{-\\infty}^{\\infty} (x-\\mu)^k f(x)dx.\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "# Percentile of a Continuous Distribution\n",
    "\n",
    "Let $p$ be a number between $0$ and $1$. The $(100p)$th percentile of the distribution\n",
    "of a continuous rv $X$, denoted by $\\eta(p)$, is defined by\n",
    "$$ p= F(\\eta(p))=\\int_{-\\infty}^{\\eta(p)} f(x)d(x).$$\n",
    "\n",
    "![](images/percentile.png)\n",
    "\n",
    "# Important Discrete RVs\n",
    "\n",
    "## Bernouli: \n",
    "\n",
    "* Any random variable whose only possible values are 0 (failure) and 1 (success). \n",
    "* $X \\sim Bernouli(p)$ $\\Rightarrow$ $P(X=0)=1-p$, $P(X=1)=p$.   \n",
    "* $E(X)=p$.\n",
    "* $Var(X)=p(1-p)$. \n",
    "\n",
    "## Binomial: \n",
    "\n",
    "* **Binomail experiment**: A sequence of $n$ idependently and identically distributed Bernouli trials. \n",
    "* The numbe of successes among the $n$  idependently and identically distributed trials. \n",
    "* $X \\sim Binomial(n,p)$.\n",
    "* $P(X=x)= {n \\choose x} p^x (1-p)^x, \\; x=0, 1, \\ldots, n$.\n",
    "* $E(X)=np$.\n",
    "* $Var(X)=np(1-p)$. \n",
    "\n",
    "# Important Discrete RVs (cont'd)\n",
    "\n",
    "* **Hypergeometric**\n",
    "\n",
    "* **Negative Binomial**\n",
    "\n",
    "* **Poisson** \n",
    "\n",
    "Review above distributions from the textbook. \n",
    "\n",
    "# Important Continuous RVs\n",
    "\n",
    "## Uniform:\n",
    "\n",
    "* $X \\sim Uniform(a,b)$.\n",
    "* $f(x;a,b)=\\frac{1}{b-a}, \\; x \\in [a,b]$.\n",
    "\n",
    "## Exponential:\n",
    "\n",
    "* $X \\sim Exp(\\lambda)$.\n",
    "* $f(x)=\\lambda e^{-\\lambda x}, \\; x\\ge 0$. \n",
    "* $F(x)=1-e^{-\\lambda x}, \\; x\\ge 0$. \n",
    "* $E(X)=\\frac{1}{\\lambda}$.\n",
    "* $Var(X)=\\frac{1}{\\lambda^2}$.\n",
    "\n",
    "and \n",
    "\n",
    "* Gamma\n",
    "\n",
    "* Chi-Squared\n",
    "\n",
    "* Weibull\n",
    "\n",
    "* Beta\n",
    "\n",
    "* Lognornmal\n",
    "\n",
    "Review above distributions from the textbook. \n",
    "\n",
    "# Normal Distribution\n",
    "\n",
    "* A continuous rv $X$ is said to have a normal distribution with parameters $\\mu$\n",
    "and $\\sigma$ (or $\\mu$ and $\\sigma^2$), where $-\\infty < \\mu < \\infty$ and $\\sigma>0$ if the pdf of $X$ is\n",
    "\\begin{equation*}\n",
    "    f(x; \\mu, \\sigma)=\\frac{1}{\\sqrt{2\\pi\\sigma}}  e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}, \\;  -\\infty < x < \\infty. \n",
    "\\end{equation*}\n",
    "\n",
    "* $X \\sim N(\\mu, \\sigma^2)$. \n",
    "\n",
    "\n",
    "# Standard Normal Distribution\n",
    "\n",
    "* The normal distribution with parameters $\\mu=0$ and $\\sigma=1$ is called *standard normal distribution*. \n",
    "* A rv having standard normal distribution is called *standard normal rv* and will be denoted by $Z$.\n",
    "\\begin{equation*}\n",
    "    f(z; 0, 1)=\\frac{1}{\\sqrt{2\\pi}}  e^{-\\frac{z^2}{2}}, \\;  -\\infty < z < \\infty. \n",
    "\\end{equation*}\n",
    "\n",
    "* The graph of $f (z; 0, 1)$ is called the standard normal (or $z$) curve. Its inflection\n",
    "points are at -1 and 1. The cdf of $Z$ is $P(Z \\le  z)=\\int_{-\\infty}^{z} f(y; 0, 1)$, which is denoted by $\\Phi(z)$.\n",
    "\n",
    "\n",
    "![](images/stnormal.png)\n",
    "\n",
    "# Percentiles of Standard Normal Distribution\n",
    "\n",
    "![](images/percentile_stnormal.png)\n",
    "\n",
    "# $z_{\\alpha}$ Notation for $z$ Critical Values\n",
    "\n",
    "* $z_{\\alpha}$ will denote the value on the $z$ axis for which $\\alpha$ of the area under the $z$ curve lies to the right of $z_{\\alpha}$. \n",
    "\n",
    "* $z_{\\alpha}$ is the $100(1-\\alpha)$th percentile of the standard normal distribution. \n",
    "\n",
    "![](images/critical_val.png)\n",
    "\n",
    "![](images/symmetry.png)\n",
    "\n",
    "# Nonstandard Normal Distribution\n",
    "\n",
    "* If $X \\sim Normal(\\mu, \\sigma^2)$, then \n",
    "$$Z=\\frac{X-\\mu}{\\sigma}$$\n",
    "has a standard normal distribution. \n",
    "\n",
    "\\begin{align*}\n",
    "    P(a \\le X \\le b) = & P(\\frac{a-\\mu}{\\sigma} \\le Z \\le \\frac{b-\\mu}{\\sigma})\\\\\n",
    "    =& \\Phi(\\frac{a-\\mu}{\\sigma}) -\\Phi(\\frac{b-\\mu}{\\sigma}).\n",
    "\\end{align*}\n",
    "\n",
    "* $P(X\\le a)=\\Phi(\\frac{a-\\mu}{\\sigma})$. \n",
    "* $P(X\\ge b)=1- \\Phi(\\frac{b-\\mu}{\\sigma})$. \n",
    "\n",
    "# Jointly Distributed Random Variables\n",
    "\n",
    "\n",
    "## Joint Probability Distribution\n",
    "\n",
    "* $p(x,y)=P(X=x, Y=y)$.\n",
    "\n",
    "\\begin{equation*}\n",
    "    P[(X,Y) \\in A]=\\begin{cases}\n",
    "        \\sum_{(x,y)} \\sum_{\\in A} p(x,y)\\\\\n",
    "        \\int_{(x,y)} \\int_{ \\in A} f(x,y) dx dy.\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "## Marginal Distribution\n",
    "\n",
    "* $p_X(x)=\\sum_{y : p(x,y)>0} p(x,y)$\n",
    "\n",
    "* $f_{X}(x)=\\int_{\\infty}^{\\infty} f(x,y)dy$\n",
    "\n",
    "## Independence\n",
    "\n",
    "Two random variables $X$ and $Y$ are said to be *independent* if for every pair of\n",
    "$x$ and $y$ values, we have \n",
    "\\begin{equation*}\n",
    "    p(x,y)=p_X(x) p_Y(y), \\; when\\  X \\ and \\ Y \\ are \\ discrete\n",
    "\\end{equation*}\n",
    "or \n",
    "\\begin{equation*}\n",
    "    f(x,y)=f_X(x) f_Y(y), \\; when\\  X \\ and \\ Y \\ are \\ continuous.\n",
    "\\end{equation*}\n",
    "Otherwise, of the above is not satisfied for all $(x,y)$, then $X$ and $Y$ are dependent. \n",
    "\n",
    "\n",
    "# More Than Two Random Variables\n",
    "\n",
    "* $p(x_1,x_2, \\ldots, x_n)=P(X_1=x_1, X_2=x_2, \\ldots, X_n=x_n)$. \n",
    "\n",
    "* $P(a_1 \\le X_1 \\le b_1, \\ldots, a_n \\le X_n \\le b_n)=\\int_{a_1}^{b_1} \\ldots \\int_{a_n}^{b_n} f(x_1, \\ldots, x_n)  dx_1 \\ldots d x_n$. \n",
    "\n",
    "* The random variables $X_1, \\ldots, X_n$ are said to be *independent* if for every subset $X_{i_1}, \\ldots, X_{i_{k}}$ of the variables (each pair, each triple, etc), the joint pmf or pdf of the subset is equal to the product of the marginakl pmf's or pdf's. \n",
    "\n",
    "# Expected Value\n",
    "\n",
    "\\begin{equation*}\n",
    "    E[h(X,Y)]=\\begin{cases}\n",
    "        \\sum_{x} \\sum_{y} h(x,y)p(x,y)\\\\\n",
    "        \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} h(x,y) f(x,y) dx dy.\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "# Covariance \n",
    "\\begin{equation*}\n",
    "    Cov(X,Y)=E[(X-\\mu_X)(Y-\\mu_Y)]=\\begin{cases}\n",
    "        \\sum_{x} \\sum_{y} (x-\\mu_X)(y-\\mu_Y)p(x,y)\\\\\n",
    "        \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} (x-\\mu_X)(y-\\mu_Y) f(x,y) dx dy.\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "*Note*: $Cov(X,Y)=E(XY)-\\mu_X \\mu_Y$. \n",
    "\n",
    "# Correlation\n",
    "\n",
    "\\begin{equation*}\n",
    "    Corr(X,Y)=\\rho_{X,Y}= \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}.\n",
    "\\end{equation*}\n",
    "\n",
    "* If $a$ and $c$ are either both positive or both negative, $Corr(aX + b, cY + d) = Corr(X, Y)$.\n",
    "* Fr any two rv's $X$ and $Y$, $-1 \\le \\rho \\le 1$. The two variables are said to be\n",
    "uncorrelated when $\\rho=0$. \n",
    "* If $X$ and $Y$ are independent, then $\\rho=0$, but $\\rho=0$ does not imply\n",
    "independence.\n",
    "* $\\rho=1$ or $\\rho=-1$ iff $Y=aX + b$ for some numbers $a$ and $b$ with $a  \\neq 0$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
